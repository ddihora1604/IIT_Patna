{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully with shape: (218805, 47)\n",
      "Sampled dataset shape: (43761, 47)\n",
      "Performing Exploratory Data Analysis...\n",
      "EDA visualizations created successfully!\n",
      "Preprocessing data...\n",
      "Starting 2-fold cross-validation with Ridor-like Classifier...\n",
      "Processing fold 1/2...\n",
      "Processing fold 2/2...\n",
      "Classification Metrics Across Folds:\n",
      "   Classifier  Fold  Class  Accuracy  Precision    Recall  F1 Score  \\\n",
      "0  Ridor-like     1      0  0.999954   0.000000  0.000000  0.000000   \n",
      "1  Ridor-like     1      1  0.994653   0.881262  0.899621  0.890347   \n",
      "2  Ridor-like     1      2  0.999954   0.000000  0.000000  0.000000   \n",
      "3  Ridor-like     1      3  0.999863   0.000000  0.000000  0.000000   \n",
      "4  Ridor-like     1      4  0.999452   0.968750  0.939394  0.953846   \n",
      "\n",
      "   Balanced Accuracy  Matthews Correlation Coefficient  Cohen Kappa Score  \\\n",
      "0           0.500000                          0.000000           0.000000   \n",
      "1           0.948312                          0.887656           0.887607   \n",
      "2           0.500000                          0.000000           0.000000   \n",
      "3           0.500000                          0.000000           0.000000   \n",
      "4           0.969605                          0.953685           0.953570   \n",
      "\n",
      "   True Positive Rate (TPR)  True Negative Rate (TNR)  \\\n",
      "0                  0.000000                  1.000000   \n",
      "1                  0.899621                  0.997003   \n",
      "2                  0.000000                  1.000000   \n",
      "3                  0.000000                  1.000000   \n",
      "4                  0.939394                  0.999816   \n",
      "\n",
      "   False Positive Rate (FPR)  False Negative Rate (FNR)  Training Time (s)  \\\n",
      "0                   0.000000                   1.000000           0.799595   \n",
      "1                   0.002997                   0.100379           0.799595   \n",
      "2                   0.000000                   1.000000           0.799595   \n",
      "3                   0.000000                   1.000000           0.799595   \n",
      "4                   0.000184                   0.060606           0.799595   \n",
      "\n",
      "   Testing Time (s)  \n",
      "0          0.099983  \n",
      "1          0.099983  \n",
      "2          0.099983  \n",
      "3          0.099983  \n",
      "4          0.099983  \n",
      "\n",
      "Average Metrics Across Folds:\n",
      "    Classifier  Class  Accuracy  Precision    Recall  F1 Score\n",
      "0   Ridor-like      0  0.999954   0.000000  0.000000  0.000000\n",
      "1   Ridor-like      1  0.994767   0.863776  0.925009  0.892837\n",
      "2   Ridor-like      2  0.999954   0.000000  0.000000  0.000000\n",
      "3   Ridor-like      3  0.999909   0.000000  0.000000  0.000000\n",
      "4   Ridor-like      4  0.999566   0.977333  0.952456  0.964728\n",
      "5   Ridor-like      5  0.999429   0.322727  0.362500  0.341270\n",
      "6   Ridor-like      6  0.985969   0.922461  0.993098  0.956342\n",
      "7   Ridor-like      7  0.999474   0.997727  0.952665  0.974618\n",
      "8   Ridor-like      8  0.999840   0.999733  0.998429  0.999081\n",
      "9   Ridor-like      9  0.999817   1.000000  0.997910  0.998954\n",
      "10  Ridor-like     10  0.996252   0.998967  0.959293  0.978445\n",
      "11  Ridor-like     11  0.999589   0.250000  0.100000  0.142857\n",
      "12  Ridor-like     12  0.999794   0.999693  0.997644  0.998667\n",
      "13  Ridor-like     13  0.999680   0.998800  0.997829  0.998314\n",
      "14  Ridor-like     14  0.998058   0.990254  0.992854  0.991528\n",
      "15  Ridor-like     15  0.999634   0.989521  0.956872  0.972833\n",
      "16  Ridor-like     16  0.997669   0.883721  0.417989  0.566129\n",
      "17  Ridor-like     17  0.999817   0.500000  0.250000  0.333333\n",
      "18  Ridor-like     18  0.998812   0.614706  0.298593  0.384091\n",
      "19  Ridor-like     19  0.999840   1.000000  0.996305  0.998147\n",
      "20  Ridor-like     20  0.999931   0.999593  0.999186  0.999389\n",
      "21  Ridor-like     21  0.999269   0.995466  0.994029  0.994723\n",
      "22  Ridor-like     22  0.997326   0.925652  0.652778  0.764782\n",
      "23  Ridor-like     23  0.999680   0.995811  0.989592  0.992691\n",
      "24  Ridor-like     24  0.999680   0.995989  0.985325  0.990622\n",
      "25  Ridor-like     25  0.999566   0.984681  0.992957  0.988798\n",
      "26  Ridor-like     26  0.998263   0.746928  0.477632  0.581960\n",
      "27  Ridor-like     27  0.998149   0.361842  0.140199  0.187943\n",
      "28  Ridor-like     28  0.999954   0.000000  0.000000  0.000000\n",
      "29  Ridor-like     29  0.998058   0.348214  0.209564  0.260117\n",
      "30  Ridor-like     30  0.999954   0.000000  0.000000  0.000000\n",
      "31  Ridor-like     31  0.999886   0.000000  0.000000  0.000000\n",
      "32  Ridor-like     32  0.999314   0.654762  0.488722  0.485714\n",
      "33  Ridor-like     33  0.999794   0.000000  0.000000  0.000000\n",
      "\n",
      "Ridor-like Classifier implementation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           balanced_accuracy_score, confusion_matrix, \n",
    "                           matthews_corrcoef, cohen_kappa_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories for output\n",
    "os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
    "os.makedirs(\"visualizations\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"rules\", exist_ok=True)\n",
    "\n",
    "# Set the number of K folds\n",
    "K_FOLDS = 2\n",
    "\n",
    "# Helper function for confusion matrix metrics\n",
    "def confusion_matrix_metrics(cm, classes):\n",
    "    metrics = {}\n",
    "    for idx, class_label in enumerate(classes):\n",
    "        TP = cm[idx, idx]  # True Positives for this class\n",
    "        FP = cm[:, idx].sum() - TP  # False Positives for this class\n",
    "        FN = cm[idx, :].sum() - TP  # False Negatives for this class\n",
    "        TN = cm.sum() - (TP + FP + FN)  # True Negatives for this class\n",
    "\n",
    "        metrics[class_label] = {\n",
    "            'TPR': TP / (TP + FN + 1e-10) if (TP + FN) > 0 else 0,\n",
    "            'TNR': TN / (TN + FP + 1e-10) if (TN + FP) > 0 else 0,\n",
    "            'FPR': FP / (FP + TN + 1e-10) if (FP + TN) > 0 else 0,\n",
    "            'FNR': FN / (FN + TP + 1e-10) if (FN + TP) > 0 else 0\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "# Custom Ridor-like Classifier implementation using Decision Trees\n",
    "class RidorLikeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_rules=6, min_samples_leaf=2, max_depth=4, random_state=42):\n",
    "        self.max_rules = max_rules\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.rules = {}\n",
    "        self.default_class = None\n",
    "        self.classes_ = None\n",
    "        self.rule_trees = []\n",
    "        self.rule_texts = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store classes\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        # Find the majority class (default class)\n",
    "        counts = np.bincount(y) if np.issubdtype(y.dtype, np.integer) else np.bincount([np.where(self.classes_ == c)[0][0] for c in y])\n",
    "        self.default_class = np.argmax(counts)\n",
    "        \n",
    "        # For each class (except default), create exception rules\n",
    "        X_curr = X.copy()\n",
    "        y_curr = y.copy()\n",
    "        \n",
    "        # Keep track of which samples are covered by rules\n",
    "        covered_indices = np.zeros(len(y), dtype=bool)\n",
    "        \n",
    "        for class_idx, class_label in enumerate(self.classes_):\n",
    "            if class_idx == self.default_class:\n",
    "                continue\n",
    "                \n",
    "            # Create binary classification problem for this class vs. others\n",
    "            y_binary = np.where(y_curr == class_label, 1, 0)\n",
    "            \n",
    "            # If enough samples of this class remain\n",
    "            if np.sum(y_binary) >= self.min_samples_leaf:\n",
    "                # Create a decision tree to find rules\n",
    "                tree = DecisionTreeClassifier(\n",
    "                    max_depth=self.max_depth,\n",
    "                    min_samples_leaf=self.min_samples_leaf,\n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "                \n",
    "                tree.fit(X_curr, y_binary)\n",
    "                self.rule_trees.append((class_label, tree))\n",
    "                \n",
    "                # Generate human-readable rules for this class\n",
    "                rule_text = f\"Class {class_label}:\\n\"\n",
    "                rule_text += self._extract_rules_from_tree(tree, X.columns if hasattr(X, 'columns') else range(X.shape[1]))\n",
    "                self.rule_texts.append(rule_text)\n",
    "                \n",
    "                # Find samples covered by these rules\n",
    "                predictions = tree.predict(X_curr)\n",
    "                newly_covered = np.where((predictions == 1) & (y_binary == 1))[0]\n",
    "                \n",
    "                # Mark these samples as covered\n",
    "                if len(newly_covered) > 0:\n",
    "                    covered_indices[newly_covered] = True\n",
    "        \n",
    "        # Save final rule set\n",
    "        for idx, rule_text in enumerate(self.rule_texts):\n",
    "            with open(f\"rules/ridor_like_rules_{idx}.txt\", \"w\") as f:\n",
    "                f.write(rule_text)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _extract_rules_from_tree(self, tree, feature_names):\n",
    "        \"\"\"Extract human-readable rules from a decision tree\"\"\"\n",
    "        tree_ = tree.tree_\n",
    "        feature_name = [\n",
    "            feature_names[i] if i != -2 else \"undefined!\"\n",
    "            for i in tree_.feature\n",
    "        ]\n",
    "        \n",
    "        rules = []\n",
    "        \n",
    "        def recurse(node, depth, path):\n",
    "            if tree_.feature[node] != -2:  # Not a leaf\n",
    "                name = feature_name[node]\n",
    "                threshold = tree_.threshold[node]\n",
    "                \n",
    "                # Left branch (<=)\n",
    "                recurse(tree_.children_left[node], depth + 1, \n",
    "                        path + [f\"{name} <= {threshold:.4f}\"])\n",
    "                \n",
    "                # Right branch (>)\n",
    "                recurse(tree_.children_right[node], depth + 1,\n",
    "                        path + [f\"{name} > {threshold:.4f}\"])\n",
    "            else:  # Leaf node\n",
    "                if tree_.value[node][0][1] > tree_.value[node][0][0]:  # More positive than negative\n",
    "                    coverage = f\"{int(tree_.value[node][0][1])}/{int(tree_.value[node][0].sum())}\"\n",
    "                    rule = \"  IF \" + \" AND \".join(path) + f\" THEN class ({coverage})\"\n",
    "                    rules.append(rule)\n",
    "        \n",
    "        recurse(0, 1, [])\n",
    "        return \"\\n\".join(rules) + \"\\n\"\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\"\"\"\n",
    "        if not hasattr(self, 'rule_trees') or len(self.rule_trees) == 0:\n",
    "            return np.full(X.shape[0], self.default_class)\n",
    "        \n",
    "        # Initialize with default class\n",
    "        predictions = np.full(X.shape[0], self.classes_[self.default_class])\n",
    "        \n",
    "        # Apply rules in order\n",
    "        for class_label, tree in self.rule_trees:\n",
    "            # Predict which samples match this rule\n",
    "            rule_predictions = tree.predict(X)\n",
    "            \n",
    "            # Update predictions where rule applies\n",
    "            mask = (rule_predictions == 1)\n",
    "            predictions[mask] = class_label\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "# Function for exploratory data analysis\n",
    "def perform_eda(df):\n",
    "    print(\"Performing Exploratory Data Analysis...\")\n",
    "      \n",
    "    \n",
    "    \n",
    "    # 1. Create histograms for numeric features\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Create histograms for each numeric feature\n",
    "        df[numeric_cols].hist(figsize=(20, 20), bins=20, layout=(7, 7))\n",
    "        plt.suptitle('Histograms of Numeric Features', y=0.92)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"visualizations/histograms.png\")\n",
    "        plt.close()\n",
    "    \n",
    "   \n",
    "    \n",
    "    # 2. Create pair plot for visualizing relationships between features\n",
    "    \n",
    "    # Select a subset of numeric columns (first 5) to avoid overloading the pair plot\n",
    "    plot_columns = list(numeric_cols[:5])\n",
    "    # Add the target variable to the plot\n",
    "    plot_columns.append('label')\n",
    "    # Create the pair plot\n",
    "    pair_plot = sns.pairplot(df[plot_columns], hue='label', diag_kind='kde', \n",
    "                          plot_kws={'alpha': 0.6, 's': 30, 'edgecolor': 'k', 'linewidth': 0.2})\n",
    "    pair_plot.fig.suptitle('Pair Plot of Features by Class', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"visualizations/pair_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    print(\"EDA visualizations created successfully!\")\n",
    "    return df  # Return potentially modified dataframe\n",
    "\n",
    "\n",
    "# Main processing function\n",
    "def main():\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_csv('C:/Users/ddihora1604/Downloads/IIT Patna/Task/Dataset 2/part-00001_preprocessed_dataset.csv')\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Take 20% of the data for faster processing (if needed)\n",
    "    df = df.sample(frac=0.2, random_state=42)\n",
    "    print(f\"Sampled dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Rename the last column as 'label' if not already named\n",
    "    df.rename(columns={df.columns[-1]: 'label'}, inplace=True)\n",
    "    \n",
    "    # Add EDA step here\n",
    "    df = perform_eda(df)\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = df['label'].values\n",
    "    X_columns = X.columns\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X_columns)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    timing_results = []\n",
    "    rules_list = []\n",
    "    \n",
    "    # Set up KFold cross-validation\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cross-validation process\n",
    "    print(f\"Starting {K_FOLDS}-fold cross-validation with Ridor-like Classifier...\")\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "        print(f\"Processing fold {fold_idx}/{K_FOLDS}...\")\n",
    "        \n",
    "        # Split the data for this fold\n",
    "        X_train, X_test = X.iloc[train_index].reset_index(drop=True), X.iloc[test_index].reset_index(drop=True)\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train Ridor-like classifier\n",
    "        ridor = RidorLikeClassifier(\n",
    "            max_rules=6,\n",
    "            min_samples_leaf=2,\n",
    "            max_depth=4,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Record training time\n",
    "        start_train_time = time.time()\n",
    "        ridor.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_train_time\n",
    "        \n",
    "        # Get the rules as text\n",
    "        rules_text = \"\\n\".join(ridor.rule_texts)\n",
    "        rules_list.append({\"Fold\": fold_idx, \"Rules\": rules_text})\n",
    "        \n",
    "        # Save rules to file\n",
    "        with open(f\"rules/fold_{fold_idx}.txt\", \"w\") as f:\n",
    "            f.write(rules_text)\n",
    "        \n",
    "        # Record prediction time\n",
    "        start_test_time = time.time()\n",
    "        y_pred = ridor.predict(X_test)\n",
    "        test_time = time.time() - start_test_time\n",
    "        \n",
    "        # Record timing info\n",
    "        timing_results.append({\n",
    "            'Classifier': 'Ridor-like',\n",
    "            'Fold': fold_idx,\n",
    "            'Training Time (s)': train_time,\n",
    "            'Testing Time (s)': test_time,\n",
    "            'Total Time (s)': train_time + test_time\n",
    "        })\n",
    "        \n",
    "        # Get unique classes\n",
    "        unique_classes = np.unique(np.concatenate([y_test, y_pred]))\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
    "        cm_metrics = confusion_matrix_metrics(cm, unique_classes)\n",
    "        \n",
    "        # Calculate metrics for each class\n",
    "        for class_label in unique_classes:\n",
    "            # Binary classification metrics for this class\n",
    "            y_test_binary = np.array([1 if y == class_label else 0 for y in y_test])\n",
    "            y_pred_binary = np.array([1 if y == class_label else 0 for y in y_pred])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            class_metrics = {\n",
    "                'Classifier': 'Ridor-like',\n",
    "                'Fold': fold_idx,\n",
    "                'Class': class_label,\n",
    "                'Accuracy': accuracy_score(y_test_binary, y_pred_binary),\n",
    "                'Precision': precision_score(y_test_binary, y_pred_binary, zero_division=0),\n",
    "                'Recall': recall_score(y_test_binary, y_pred_binary),\n",
    "                'F1 Score': f1_score(y_test_binary, y_pred_binary),\n",
    "                'Balanced Accuracy': balanced_accuracy_score(y_test_binary, y_pred_binary),\n",
    "                'Matthews Correlation Coefficient': matthews_corrcoef(y_test_binary, y_pred_binary),\n",
    "                'Cohen Kappa Score': cohen_kappa_score(y_test_binary, y_pred_binary),\n",
    "                'True Positive Rate (TPR)': cm_metrics[class_label]['TPR'],\n",
    "                'True Negative Rate (TNR)': cm_metrics[class_label]['TNR'],\n",
    "                'False Positive Rate (FPR)': cm_metrics[class_label]['FPR'],\n",
    "                'False Negative Rate (FNR)': cm_metrics[class_label]['FNR'],\n",
    "                'Training Time (s)': train_time,\n",
    "                'Testing Time (s)': test_time\n",
    "            }\n",
    "            \n",
    "            results.append(class_metrics)\n",
    "        \n",
    "        # Plot and save confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "        plt.title(f\"Ridor-like Classifier - Fold {fold_idx} Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"confusion_matrices/fold_{fold_idx}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Visualize class distribution in test set\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        class_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "        sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "        plt.title(f\"Class Distribution in Test Set (Fold {fold_idx})\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"visualizations/class_distribution_fold_{fold_idx}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Feature importance for this fold's model\n",
    "        if hasattr(ridor, 'rule_trees') and len(ridor.rule_trees) > 0:\n",
    "            # Collect feature importance from all trees\n",
    "            feature_importance = np.zeros(len(X_columns))\n",
    "            for _, tree in ridor.rule_trees:\n",
    "                if hasattr(tree, 'feature_importances_'):\n",
    "                    feature_importance += tree.feature_importances_\n",
    "            \n",
    "            if np.sum(feature_importance) > 0:\n",
    "                # Normalize\n",
    "                feature_importance = feature_importance / len(ridor.rule_trees)\n",
    "                \n",
    "                # Create feature importance plot\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': X_columns,\n",
    "                    'Importance': feature_importance\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                sns.barplot(x='Importance', y='Feature', data=importance_df[:15])  # Top 15 features\n",
    "                plt.title(f'Feature Importance (Fold {fold_idx})')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"visualizations/feature_importance_fold_{fold_idx}.png\")\n",
    "                plt.close()\n",
    "    \n",
    "    # Create DataFrames for results and save to CSV\n",
    "    timing_df = pd.DataFrame(timing_results)\n",
    "    timing_df.to_csv(\"results/timing.csv\", index=False)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"Classification Metrics Across Folds:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"results/metrics.csv\", index=False)\n",
    "    \n",
    "    # Save rules to CSV\n",
    "    rules_df = pd.DataFrame(rules_list)\n",
    "    rules_df.to_csv(\"results/rules.csv\", index=False)\n",
    "    \n",
    "    # Calculate and display average metrics across folds\n",
    "    avg_metrics = results_df.groupby(['Classifier', 'Class']).mean().reset_index()\n",
    "    avg_metrics.to_csv(\"results/avg_metrics.csv\", index=False)\n",
    "    print(\"\\nAverage Metrics Across Folds:\")\n",
    "    print(avg_metrics[['Classifier', 'Class', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "    \n",
    "    \n",
    "    print(\"\\nRidor-like Classifier implementation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
