{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully with shape: (218805, 47)\n",
      "Sampled dataset shape: (43761, 47)\n",
      "Preprocessing data...\n",
      "Starting 2-fold cross-validation with Ridge Classifier...\n",
      "Processing fold 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ddihora1604\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics Across Folds:\n",
      "        Classifier  Fold  Class  Accuracy  Precision    Recall  F1 Score  \\\n",
      "0  RidgeClassifier     1      0  0.999954   0.000000  0.000000  0.000000   \n",
      "1  RidgeClassifier     1      1  0.984416   0.644961  0.787879  0.709292   \n",
      "2  RidgeClassifier     1      2  0.999954   0.000000  0.000000  0.000000   \n",
      "3  RidgeClassifier     1      3  0.999863   0.000000  0.000000  0.000000   \n",
      "4  RidgeClassifier     1      4  0.988575   0.310897  0.734848  0.436937   \n",
      "\n",
      "   Balanced Accuracy  Matthews Correlation Coefficient  Cohen Kappa Score  \\\n",
      "0           0.500000                          0.000000           0.000000   \n",
      "1           0.888577                          0.705078           0.701367   \n",
      "2           0.500000                          0.000000           0.000000   \n",
      "3           0.500000                          0.000000           0.000000   \n",
      "4           0.862481                          0.473511           0.432122   \n",
      "\n",
      "   True Positive Rate (TPR)  True Negative Rate (TNR)  \\\n",
      "0                  0.000000                  1.000000   \n",
      "1                  0.787879                  0.989276   \n",
      "2                  0.000000                  1.000000   \n",
      "3                  0.000000                  1.000000   \n",
      "4                  0.734848                  0.990114   \n",
      "\n",
      "   False Positive Rate (FPR)  False Negative Rate (FNR)  Training Time (s)  \\\n",
      "0                   0.000000                   1.000000           0.032361   \n",
      "1                   0.010724                   0.212121           0.032361   \n",
      "2                   0.000000                   1.000000           0.032361   \n",
      "3                   0.000000                   1.000000           0.032361   \n",
      "4                   0.009886                   0.265152           0.032361   \n",
      "\n",
      "   Testing Time (s)  \n",
      "0          0.006204  \n",
      "1          0.006204  \n",
      "2          0.006204  \n",
      "3          0.006204  \n",
      "4          0.006204  \n",
      "\n",
      "Average Metrics Across Folds:\n",
      "         Classifier  Class  Accuracy  Precision    Recall  F1 Score\n",
      "0   RidgeClassifier      0  0.999954   0.000000  0.000000  0.000000\n",
      "1   RidgeClassifier      1  0.985215   0.648169  0.816558  0.722522\n",
      "2   RidgeClassifier      2  0.999954   0.000000  0.000000  0.000000\n",
      "3   RidgeClassifier      3  0.999909   0.000000  0.000000  0.000000\n",
      "4   RidgeClassifier      4  0.990951   0.155449  0.367424  0.218468\n",
      "5   RidgeClassifier      5  0.999589   0.000000  0.000000  0.000000\n",
      "6   RidgeClassifier      6  0.999177   0.995874  0.998814  0.997342\n",
      "7   RidgeClassifier      7  0.998378   0.898745  0.954755  0.925680\n",
      "8   RidgeClassifier      8  0.997646   0.975646  0.997896  0.986645\n",
      "9   RidgeClassifier      9  0.999886   1.000000  0.998702  0.999351\n",
      "10  RidgeClassifier     10  0.941249   0.608044  0.986596  0.752280\n",
      "11  RidgeClassifier     11  0.999634   0.000000  0.000000  0.000000\n",
      "12  RidgeClassifier     12  0.946276   0.677302  0.578225  0.623737\n",
      "13  RidgeClassifier     13  0.942574   0.622923  0.998794  0.767299\n",
      "14  RidgeClassifier     14  0.941957   0.664788  0.998611  0.798188\n",
      "15  RidgeClassifier     15  0.992550   0.220149  0.427536  0.290640\n",
      "16  RidgeClassifier     16  0.996389   0.000000  0.000000  0.000000\n",
      "17  RidgeClassifier     17  0.999886   1.000000  0.464286  0.633333\n",
      "18  RidgeClassifier     18  0.998697   0.250000  0.043478  0.074074\n",
      "19  RidgeClassifier     19  0.956445   0.156250  0.002643  0.005198\n",
      "20  RidgeClassifier     20  0.944494   0.000000  0.000000  0.000000\n",
      "21  RidgeClassifier     21  0.945134   0.942825  0.222897  0.360554\n",
      "22  RidgeClassifier     22  0.993922   0.738095  0.127551  0.216916\n",
      "23  RidgeClassifier     23  0.983456   0.570517  0.998960  0.726223\n",
      "24  RidgeClassifier     24  0.983250   0.000000  0.000000  0.000000\n",
      "25  RidgeClassifier     25  0.998835   0.946025  0.996479  0.970597\n",
      "26  RidgeClassifier     26  0.997372   0.000000  0.000000  0.000000\n",
      "27  RidgeClassifier     27  0.998263   0.750000  0.040199  0.076276\n",
      "28  RidgeClassifier     28  0.999977   0.000000  0.000000  0.000000\n",
      "29  RidgeClassifier     29  0.998195   0.000000  0.000000  0.000000\n",
      "30  RidgeClassifier     30  0.999954   0.000000  0.000000  0.000000\n",
      "31  RidgeClassifier     31  0.999954   0.000000  0.000000  0.000000\n",
      "32  RidgeClassifier     32  0.999246   0.000000  0.000000  0.000000\n",
      "33  RidgeClassifier     33  0.999886   0.000000  0.000000  0.000000\n",
      "\n",
      "Ridge Classifier implementation completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            balanced_accuracy_score, confusion_matrix,\n",
    "                            matthews_corrcoef, cohen_kappa_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create directory for visualizations\n",
    "os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
    "os.makedirs(\"visualizations\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Set the number of K folds\n",
    "K_FOLDS = 2\n",
    "\n",
    "# Helper function for confusion matrix metrics\n",
    "def confusion_matrix_metrics(cm, classes):\n",
    "    metrics = {}\n",
    "    for idx, class_label in enumerate(classes):\n",
    "        TP = cm[idx, idx]  # True Positives for this class\n",
    "        FP = cm[:, idx].sum() - TP  # False Positives for this class\n",
    "        FN = cm[idx, :].sum() - TP  # False Negatives for this class\n",
    "        TN = cm.sum() - (TP + FP + FN)  # True Negatives for this class\n",
    "\n",
    "        metrics[class_label] = {\n",
    "            'TPR': TP / (TP + FN + 1e-10) if (TP + FN) > 0 else 0,\n",
    "            'TNR': TN / (TN + FP + 1e-10) if (TN + FP) > 0 else 0,\n",
    "            'FPR': FP / (FP + TN + 1e-10) if (FP + TN) > 0 else 0,\n",
    "            'FNR': FN / (FN + TP + 1e-10) if (FN + TP) > 0 else 0\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv('C:/Users/ddihora1604/Downloads/IIT Patna/Task/Dataset 2/part-00001_preprocessed_dataset.csv')\n",
    "    print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Take 20% of the data for faster processing (if needed)\n",
    "df = df.sample(frac=0.2, random_state=42)\n",
    "print(f\"Sampled dataset shape: {df.shape}\")\n",
    "\n",
    "# Rename the last column as 'label' if not already named\n",
    "df.rename(columns={df.columns[-1]: 'label'}, inplace=True)\n",
    "\n",
    "# Data preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = df.drop(columns=['label'])\n",
    "X_columns = X.columns\n",
    "X = imputer.fit_transform(X)\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "timing_results = []\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Ridge classifier with hyperparameters (removed deprecated 'normalize' parameter)\n",
    "ridge_classifier = RidgeClassifier(\n",
    "    alpha=1.0,  # Regularization strength\n",
    "    fit_intercept=True,\n",
    "    copy_X=True,\n",
    "    max_iter=1000,\n",
    "    tol=0.001,\n",
    "    class_weight=None,\n",
    "    solver='auto',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation process\n",
    "print(f\"Starting {K_FOLDS}-fold cross-validation with Ridge Classifier...\")\n",
    "fold_idx = 1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"Processing fold {fold_idx}/{K_FOLDS}...\")\n",
    "    \n",
    "    # Split the data for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Record training time\n",
    "    start_train_time = time.time()\n",
    "    ridge_classifier.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_train_time\n",
    "    \n",
    "    # Record prediction time\n",
    "    start_test_time = time.time()\n",
    "    y_pred = ridge_classifier.predict(X_test)\n",
    "    test_time = time.time() - start_test_time\n",
    "    \n",
    "    # Record timing info\n",
    "    timing_results.append({\n",
    "        'Classifier': 'RidgeClassifier',\n",
    "        'Fold': fold_idx,\n",
    "        'Training Time (s)': train_time,\n",
    "        'Testing Time (s)': test_time,\n",
    "        'Total Time (s)': train_time + test_time\n",
    "    })\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    unique_classes = np.unique(y)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
    "    cm_metrics = confusion_matrix_metrics(cm, unique_classes)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    for class_label in unique_classes:\n",
    "        # Binary classification metrics for this class\n",
    "        y_test_binary = (y_test == class_label).astype(int)\n",
    "        y_pred_binary = (y_pred == class_label).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        class_metrics = {\n",
    "            'Classifier': 'RidgeClassifier',\n",
    "            'Fold': fold_idx,\n",
    "            'Class': class_label,\n",
    "            'Accuracy': accuracy_score(y_test_binary, y_pred_binary),\n",
    "            'Precision': precision_score(y_test_binary, y_pred_binary, zero_division=0),\n",
    "            'Recall': recall_score(y_test_binary, y_pred_binary),\n",
    "            'F1 Score': f1_score(y_test_binary, y_pred_binary),\n",
    "            'Balanced Accuracy': balanced_accuracy_score(y_test_binary, y_pred_binary),\n",
    "            'Matthews Correlation Coefficient': matthews_corrcoef(y_test_binary, y_pred_binary),\n",
    "            'Cohen Kappa Score': cohen_kappa_score(y_test_binary, y_pred_binary),\n",
    "            'True Positive Rate (TPR)': cm_metrics[class_label]['TPR'],\n",
    "            'True Negative Rate (TNR)': cm_metrics[class_label]['TNR'],\n",
    "            'False Positive Rate (FPR)': cm_metrics[class_label]['FPR'],\n",
    "            'False Negative Rate (FNR)': cm_metrics[class_label]['FNR'],\n",
    "            'Training Time (s)': train_time,\n",
    "            'Testing Time (s)': test_time\n",
    "        }\n",
    "        \n",
    "        results.append(class_metrics)\n",
    "    \n",
    "    # Plot and save confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_classes, yticklabels=unique_classes)\n",
    "    plt.title(f\"Ridge Classifier - Fold {fold_idx} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_matrices/fold_{fold_idx}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance visualization (coefficients of Ridge Classifier)\n",
    "    if fold_idx == 1:  # Only for the first fold to avoid redundancy\n",
    "        coef = ridge_classifier.coef_\n",
    "        # For multiclass, average the absolute coefficients across classes\n",
    "        if coef.ndim > 1:\n",
    "            importance = np.mean(np.abs(coef), axis=0)\n",
    "        else:\n",
    "            importance = np.abs(coef)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_columns,\n",
    "            'Importance': importance\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top 20 features or all if less than 20\n",
    "        top_n = min(20, len(feature_importance))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(top_n))\n",
    "        plt.title('Ridge Classifier Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"visualizations/feature_importance.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Pair plot for the first 5 features and target\n",
    "    if fold_idx == 1:  # Only for the first fold\n",
    "        # Convert numpy array back to DataFrame for visualization\n",
    "        sample_size = min(1000, X_test.shape[0])  # Limit sample size for visualization\n",
    "        vis_df = pd.DataFrame(X_test[:sample_size], columns=X_columns)\n",
    "        vis_df['label'] = y_test[:sample_size]\n",
    "        \n",
    "        # Select first 5 features (or less if fewer features exist)\n",
    "        plot_features = list(X_columns[:min(5, len(X_columns))])\n",
    "        plot_features.append('label')\n",
    "        \n",
    "        # Create pair plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        pair_plot = sns.pairplot(vis_df[plot_features], hue='label', height=2.5)\n",
    "        plt.suptitle('Pair Plot of Top Features by Ridge Classifier', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        pair_plot.savefig(\"visualizations/pair_plot.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    fold_idx += 1\n",
    "\n",
    "# Create DataFrames for results and save to CSV\n",
    "timing_df = pd.DataFrame(timing_results)\n",
    "timing_df.to_csv(\"results/timing.csv\", index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Classification Metrics Across Folds:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"results/metrics.csv\", index=False)\n",
    "\n",
    "# Calculate and display average metrics across folds\n",
    "avg_metrics = results_df.groupby(['Classifier', 'Class']).mean().reset_index()\n",
    "avg_metrics.to_csv(\"results/avg_metrics.csv\", index=False)\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(avg_metrics[['Classifier', 'Class', 'Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "\n",
    "print(\"\\nRidge Classifier implementation completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
